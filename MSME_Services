import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.model_selection import LeaveOneOut, GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
 
# Define the weight dictionaries and scoring criteria (unchanged)
metric_weights = {
    "Financial Metrics": 0.30,
    "Banking & Creditworthiness": 0.40,
    "Business Health & Risk Factors": 0.30
}
 
submetric_weights = {
    "Financial Metrics": {
        "Revenue Growth": 0.08,
        "EBITDA Margin": 0.05,
        "Receivables Turnover": 0.03,
        "ROE": 0.05,
        "ROCE": 0.05,
        "DSO": 0.04
    },
    "Banking & Creditworthiness": {
        "Quick Ratio": 0.10,
        "Current Ratio": 0.08,
        "D/E Ratio": 0.08,
        "Interest Coverage": 0.06,
        "DSR": 0.08
    },
    "Business Health & Risk Factors": {
        "Business Vintage": 0.06,
        "Customer Profile": 0.06,
        "Industry Outlook": 0.08,
        "Altman Z-Score": 0.10
    }
}
 
scoring_criteria = {
 
    "Financial Metrics": {
        "Revenue Growth": [(14, float('inf'), 5), (10, 14, 4), (6, 10, 3), (3, 6, 2), (float('-inf'), 3, 1)],
        "EBITDA Margin": [(18, float('inf'), 5), (12, 18, 4), (8, 12, 3), (5, 8, 2), (float('-inf'), 5, 1)],
        "Receivables Turnover": [(8, float('inf'), 5), (6, 8, 4), (4, 6, 3), (2, 4, 2), (float('-inf'), 2, 1)],
        "ROE": [(15, float('inf'), 5), (10, 15, 4), (6, 10, 3), (3, 6, 2), (float('-inf'), 3, 1)],
        "ROCE": [(15, float('inf'), 5), (10, 15, 4), (6, 10, 3), (3, 6, 2), (float('-inf'), 3, 1)],
        "DSO": [(float('-inf'), 30, 5), (30, 45, 4), (45, 60, 3), (60, 90, 2), (90, float('inf'), 1)]
    },
    "Banking & Creditworthiness": {
        "Quick Ratio": [(2.0, float('inf'), 5), (1.5, 2.0, 4), (1.2, 1.5, 3), (1.0, 1.2, 2), (float('-inf'), 1.0, 1)],
        "Current Ratio": [(2.5, float('inf'), 5), (2.0, 2.5, 4), (1.5, 2.0, 3), (1.0, 1.5, 2), (float('-inf'), 1.0, 1)],
        "D/E Ratio": [(float('-inf'), 1.0, 5), (1.0, 2.0, 4), (2.0, 3.0, 3), (3.0, 4.0, 2), (4.0, float('inf'), 1)],
        "Interest Coverage": [(4.0, float('inf'), 5), (2.5, 4.0, 4), (1.5, 2.5, 3), (1.0, 1.5, 2), (float('-inf'), 1.0, 1)],
        "DSR": [(3.5, float('inf'), 5), (2.5, 3.5, 4), (1.5, 2.5, 3), (1.0, 1.5, 2), (float('-inf'), 1.0, 1)]
    },
    "Business Health & Risk Factors": {
        "Business Vintage": [(10, float('inf'), 5), (7, 10, 4), (4, 7, 3), (2, 4, 2), (float('-inf'), 2, 1)],
        "Customer Profile": {"Large Clients": 5, "Mid-Sized Clients": 4, "Mixed Clients": 3, "Small Clients": 2, "Unstable Clients": 1},
        "Industry Outlook": {"Strong Growth": 5, "Moderate Growth": 4, "Stable": 3, "Declining": 2, "Severe Decline": 1},
        "Altman Z-Score": [(3.0, float('inf'), 5), (2.5, 3.0, 4), (1.8, 2.5, 3), (1.0, 1.8, 2), (float('-inf'), 1.0, 1)]
    }
}
 
# Mapping dictionaries for categorical variables
categorical_mappings = {
    "Customer Profile": {
        "Large Clients": 5,
        "Mid-Sized Clients": 4,
        "Mixed Clients": 3,
        "Small Clients": 2,
        "Unstable Clients": 1
    },
    "Industry Outlook": {
        "Strong Growth": 5,
        "Moderate Growth": 4,
        "Stable": 3,
        "Declining": 2,
        "Severe Decline": 1
    }
}
 
# Define category metrics mapping
category_metrics = {
    "Financial Metrics": [
        "revenue growth", "ebitda margin", "receivable turnover",
        "roe", "roce", "dso"
    ],
    "Banking & Creditworthiness": [
        "quick ratio", "current ratio", "d/e ratio",
        "interest coverage ratio", "dsr"
    ],
    "Business Health & Risk Factors": [
        "business vintage", "altman z score",
        "customer profile", "industry outlook"
    ]
}
 
# Create a mapping between column names and scoring criteria keys
column_to_criteria_key = {
    "revenue growth": "Revenue Growth",
    "ebitda margin": "EBITDA Margin",
    "receivable turnover": "Receivables Turnover",
    "roe": "ROE",
    "roce": "ROCE",
    "dso": "DSO",
    "quick ratio": "Quick Ratio",
    "current ratio": "Current Ratio",
    "d/e ratio": "D/E Ratio",
    "interest coverage ratio": "Interest Coverage",
    "dsr": "DSR",
    "business vintage": "Business Vintage",
    "altman z score": "Altman Z-Score",
    "customer profile": "Customer Profile",
    "industry outlook": "Industry Outlook"
}
 
def score_metric(category, metric, value):
    """Score a metric based on its value and criteria."""
    if pd.isna(value):
        return 3  # Default middle score for missing values
 
    # Map column name to criteria key
    criteria_key = column_to_criteria_key.get(metric, metric)
 
    # Handle categorical metrics
    if criteria_key in ["Customer Profile", "Industry Outlook"]:
        if isinstance(value, str):
            # For string values, get the score from the dictionary
            return scoring_criteria[category][criteria_key].get(value, 3)
        else:
            # If already numeric (mapped), return as is if in range 1-5
            return min(5, max(1, int(value)))
    # Handle numeric metrics
    else:
        try:
            numeric_value = float(value)
        except (ValueError, TypeError):
            return 3  # Default to middle score for invalid values
 
        for lower, upper, score in scoring_criteria[category][criteria_key]:
            if lower <= numeric_value < upper:
                return score
        return 1  # Default lowest score if no criteria match
 
def compute_ahp_score(row):
    """Compute the AHP score for a row of data."""
    total_score = 0
    total_applied_weight = 0
 
    for category, category_weight in metric_weights.items():
        category_score = 0
        category_applied_weight = 0
 
        for metric in category_metrics.get(category, []):
            if metric in row.index and not pd.isna(row[metric]):
                criteria_key = column_to_criteria_key.get(metric, metric)
                metric_weight = submetric_weights[category].get(criteria_key, 0)
 
                try:
                    metric_score = score_metric(category, metric, row[metric])
                    category_score += metric_score * metric_weight
                    category_applied_weight += metric_weight
                except Exception as e:
                    continue
 
        if category_applied_weight > 0:
            normalized_category_score = category_score / category_applied_weight
            total_score += normalized_category_score * category_weight
            total_applied_weight += category_weight
 
    return total_score / total_applied_weight if total_applied_weight > 0 else 3
 
def clean_extreme_outliers(df, columns, threshold=5):
    """Clean extreme outliers using IQR method with adjustable threshold."""
    df_cleaned = df.copy()
 
    for col in columns:
        if col in df.columns and df[col].dtype != 'object':
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
 
            lower_bound = q1 - threshold * iqr
            upper_bound = q3 + threshold * iqr
 
            # Replace outliers with NaN
            mask = (df[col] < lower_bound) | (df[col] > upper_bound)
            if mask.any():
                df_cleaned.loc[mask, col] = np.nan
 
    return df_cleaned
 
def remove_high_correlation(df, threshold=0.95):
    """Remove highly correlated features."""
    corr_matrix = df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
 
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    return df.drop(to_drop, axis=1)
 
data_str = """company name\trevenue growth\tebitda margin\treceivables turnover\troe\troce\tdso\tquick ratio\tcurrent ratio\td/e ratio\tinterest coverage ratio\tdsr\tbusiness vintage\taltman z score\tcustomer profile\tindustry outlook
accurate meters ltd.\t-82.894737\t18.461538\t0.437637\t0.32077\t0.251889\t834.025\t0.155116\t2.20132\t0.161859\t1.066667\t1.466667\t40\t0.794170873\tmid-sized clients\tstable
accurate transformers ltd.\t-5.494075\t17.797927\t1.267793\t0.953924\t0.240303\t287.901935\t0.975855\t1.246777\t3.015404\t1.020177\t0.175461\t37\t1.389663814\tmid-sized clients\tstable
accuracy shipping ltd.\t-18.599932\t4.801389\t6.361528\t0.399948\t0.200074\t57.376149\t1.000552\t1.410016\t0.996617\t1.030222\t0.261458\t17\t3.82\tmid-sized clients\tstable
ace pipeline contracts pvt. ltd.\t58.833248\t11.908188\t20.919996\t30.581597\t20.996484\t17.447422\t1.8304\t1.95285\t0.34416\t10.576923\t1.016505\t36\t3.553246984\tmid-sized clients\tstable
ace tyres pvt. ltd.\t-12.08351\t30.315923\t7.13647\t6.950584\t6.704216\t51.145732\t10.557884\t11.274751\t0.013809\t69.647727\t8.309904\t24\t1.682847969\tmid-sized clients\tstable
acuite ratings & research ltd.\t-2.712625\t38.930818\t1196\t15.708785\t15.708785\t0.305184\t1.338652\t1.339166\t0\t1561\t1261\t20\t1.785317625\tmid-sized clients\tstable
adage automation pvt. ltd.\t30.17714\t14.271829\t4.02554\t29.671272\t19.539437\t90.671059\t0.954175\t1.629182\t0.662521\t6.547368\t0.772303\t24\t2.327087557\tmid-sized clients\tstable"""
 
def process_dataset():
    """Process financial data and build the model."""
    # 1. Load and preprocess the data
    rows = data_str.strip().split('\n')
    headers = [h.strip().lower() for h in rows[0].split('\t')]
 
    data = []
    for row in rows[1:]:
        values = row.split('\t')
        row_dict = {headers[i]: values[i] for i in range(min(len(headers), len(values)))}
        data.append(row_dict)
 
    df = pd.DataFrame(data)
 
    # 2. Clean and prepare data
    special_values = ['Not Available', 'Not Applicable', 'Missing', 'Not applicable']
    df = df.replace(special_values, np.nan)
 
    # Clean numeric columns
    for col in df.columns:
        if col not in ['company name', 'customer profile', 'industry outlook']:
            if df[col].dtype == 'object':
                df[col] = df[col].str.replace('%', '', regex=False)
                df[col] = df[col].str.replace('−', '-', regex=False)
                df[col] = df[col].str.replace(',', '', regex=False)
                df[col] = df[col].str.replace(' Years', '', regex=False)
                df[col] = df[col].str.replace(' Year', '', regex=False)
 
            df[col] = pd.to_numeric(df[col], errors='coerce')
 
    # Check for and handle extreme outliers - likely to be a data entry error
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    df = clean_extreme_outliers(df, numeric_cols, threshold=10)
 
    # Specifically check the revenue growth column for extreme values
    if 'revenue growth' in df.columns:
        # This handles the problematic 26,102.27 value (typo)
        # Set a reasonable cap based on domain knowledge
        reasonable_max = 100  # Assume max 100% growth
        df.loc[df['revenue growth'] > reasonable_max, 'revenue growth'] = np.nan
 
    # 3. Apply categorical mappings
    for col, mapping in categorical_mappings.items():
        col_lower = col.lower()
        if col_lower in df.columns:
            df[col_lower] = df[col_lower].map(lambda x: mapping.get(x, 3) if isinstance(x, str) else x)
 
    # 4. Identify features for modeling
    non_feature_cols = ['company', 'company name']
    feature_cols = [col for col in df.columns if col.lower() not in non_feature_cols]
 
    numeric_features = []
    for col in feature_cols:
        try:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            if df[col].notna().any():
                numeric_features.append(col)
        except:
            continue
 
    # 5. Calculate target AHP score
    df['ahp_score'] = df.apply(compute_ahp_score, axis=1)
 
    # 6. Create train and test sets
    X = df[feature_cols].copy()
    y = df['ahp_score'].copy()
 
    # Handle missing values using KNN imputation
    imputer = KNNImputer(n_neighbors=3)
    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
 
    # Remove highly correlated features
    X_filtered = remove_high_correlation(X_imputed, threshold=0.95)
   
    # Store the feature names after correlation filtering
    filtered_features = X_filtered.columns.tolist()
 
    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X_filtered, y, test_size=0.3, random_state=42
    )
 
    # 7. Feature preprocessing pipeline
    numeric_transformer = Pipeline(steps=[
        ('scaler', RobustScaler()),
        ('selector', SelectKBest(f_regression, k=min(8, len(X_filtered.columns))))
    ])
 
    # 8. Model building and evaluation
    model = XGBRegressor(
        n_estimators=100,
        learning_rate=0.05,
        max_depth=3,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
 
    # Train the model
    model.fit(X_train, y_train)
 
    # Evaluate the model
    y_pred = model.predict(X_test)
 
    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
 
    print(f"Model Performance:")
    print(f"RMSE: {rmse:.4f}")
    print(f"R2 Score: {r2:.4f}")
    print(f"MAE: {mae:.4f}")
 
    # 9. Feature importance analysis
    feature_importance = model.feature_importances_
    importance_df = pd.DataFrame({
        'Feature': X_filtered.columns,
        'Importance': feature_importance
    }).sort_values('Importance', ascending=False)
 
    print("\nFeature Importance:")
    print(importance_df)
 
    # 10. Alternative model comparison
    # Try RandomForest as an alternative model
    rf_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=5,
        random_state=42
    )
 
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
 
    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
    rf_r2 = r2_score(y_test, rf_pred)
 
    print("\nRandom Forest Performance:")
    print(f"RMSE: {rf_rmse:.4f}")
    print(f"R2 Score: {rf_r2:.4f}")

    # 11. Visualization of predicted vs actual values
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.7)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
    plt.xlabel('Actual AHP Score')
    plt.ylabel('Predicted AHP Score')
    plt.title('XGBoost: Actual vs Predicted AHP Scores')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
 
    # 12. Hyperparameter tuning with cross-validation
    param_grid = {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [2, 3, 4, 5],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9]
    }
 
    # Fix the RandomizedSearchCV implementation
    try:
        random_search = RandomizedSearchCV(
            estimator=XGBRegressor(random_state=42),
            param_distributions=param_grid,
            n_iter=10,  # Reduced from 20 to speed up execution
            scoring='neg_mean_squared_error',
            cv=3,  # Reduced from 5 to speed up execution
            verbose=1,
            random_state=42,
            n_jobs=1  # Changed from -1 to avoid potential parallel processing issues
        )
 
        random_search.fit(X_filtered, y)
 
        print("\nBest Hyperparameters:")
        print(random_search.best_params_)
        print(f"Best Cross-Validation Score (RMSE): {np.sqrt(-random_search.best_score_):.4f}")
 
        # 13. Evaluate best model
        best_model = random_search.best_estimator_
        best_pred = best_model.predict(X_test)
        best_rmse = np.sqrt(mean_squared_error(y_test, best_pred))
        best_r2 = r2_score(y_test, best_pred)
 
        print("\nTuned Model Performance:")
        print(f"RMSE: {best_rmse:.4f}")
        print(f"R2 Score: {best_r2:.4f}")
    except Exception as e:
        print(f"\nError during hyperparameter tuning: {str(e)}")
        print("Using the base XGBoost model instead.")
        best_model = model
        best_rmse = rmse
        best_r2 = r2
 
    # 14. Save the best model
    try:
        import joblib
        joblib.dump(best_model, 'ahp_prediction_model.pkl')
        print("\nModel saved as 'ahp_prediction_model.pkl'")
    except Exception as e:
        print(f"\nError saving model: {str(e)}")
 
    # Keep track of the feature columns used for prediction
    model_features = filtered_features
 
    # 15. Function to predict AHP score for new companies
    def predict_ahp_score(new_data):
        """
        Predict AHP score for new company data.
 
        Parameters:
        new_data (dict): Dictionary with company metrics as keys and values
 
        Returns:
        float: Predicted AHP score
        """
        # Convert to DataFrame
        new_df = pd.DataFrame([new_data])
 
        # Apply same preprocessing as training data
        for col, mapping in categorical_mappings.items():
            col_lower = col.lower()
            if col_lower in new_df.columns:
                new_df[col_lower] = new_df[col_lower].map(lambda x: mapping.get(x, 3) if isinstance(x, str) else x)
 
        # Create a DataFrame with all original features for imputation
        all_features_df = pd.DataFrame(columns=feature_cols)
        for col in feature_cols:
            if col in new_data:
                all_features_df[col] = [new_data[col]]
            else:
                all_features_df[col] = [np.nan]
       
        # Apply imputation to all features
        all_features_imputed = pd.DataFrame(
            imputer.transform(all_features_df),
            columns=feature_cols
        )
       
        # Select only the filtered features used in the model
        model_input = all_features_imputed[filtered_features]
 
        # Make prediction
        prediction = best_model.predict(model_input)[0]
 
        return prediction
 
    return best_model, imputer, feature_cols, filtered_features, predict_ahp_score
 
if __name__ == "__main__":
 
    model, imputer, all_features, filtered_features, predict_function = process_dataset()
 
    # Get the original dataset
    rows = data_str.strip().split('\n')
    headers = [h.strip().lower() for h in rows[0].split('\t')]
 
    data = []
    for row in rows[1:]:
        values = row.split('\t')
        row_dict = {headers[i]: values[i] for i in range(min(len(headers), len(values)))}
        data.append(row_dict)
 
    df = pd.DataFrame(data)
 
    # Clean and preprocess the same way as in process_dataset
    special_values = ['Not Available', 'Not Applicable', 'Missing', 'Not applicable']
    df = df.replace(special_values, np.nan)
 
    # Clean numeric columns
    for col in df.columns:
        if col not in ['company name', 'customer profile', 'industry outlook']:
            if df[col].dtype == 'object':
                df[col] = df[col].str.replace('%', '', regex=False)
                df[col] = df[col].str.replace('−', '-', regex=False)
                df[col] = df[col].str.replace(',', '', regex=False)
                df[col] = df[col].str.replace(' Years', '', regex=False)
                df[col] = df[col].str.replace(' Year', '', regex=False)
 
            df[col] = pd.to_numeric(df[col], errors='coerce')
 
    # Apply categorical mappings
    for col, mapping in categorical_mappings.items():
        col_lower = col.lower()
        if col_lower in df.columns:
            df[col_lower] = df[col_lower].map(lambda x: mapping.get(x, 3) if isinstance(x, str) else x)
 
    # Calculate actual AHP scores
    df['actual_ahp_score'] = df.apply(compute_ahp_score, axis=1)
 
    # Print model features to debug
    print("\nAll features used for imputation:")
    print(all_features)
   
    print("\nFiltered features used for model training:")
    print(filtered_features)
   
    print("\nColumns in the test dataframe:")
    print(df.columns.tolist())
 
    # Predict AHP scores for all companies
    results = []
    for idx, row in df.iterrows():
        company_name = row['company name']
 
        # Create company data dictionary with all available features
        company_data = {}
        for col in df.columns:
            if col != 'actual_ahp_score' and col != 'company name':
                company_data[col] = row[col]
 
        try:
            predicted_score = predict_function(company_data)
            actual_score = row['actual_ahp_score']
            results.append({
                'Company': company_name,
                'Predicted Score': predicted_score,
                'Actual Score': actual_score,
                'Difference': abs(predicted_score - actual_score)
            })
        except Exception as e:
            print(f"Error predicting for {company_name}: {str(e)}")
            continue
 
    # Display results
    if results:
        results_df = pd.DataFrame(results)
        print("\nPredictions for all companies in dataset:")
        print(results_df)
 
        # Calculate overall metrics
        rmse = np.sqrt(mean_squared_error(results_df['Actual Score'], results_df['Predicted Score']))
        r2 = r2_score(results_df['Actual Score'], results_df['Predicted Score'])
        mae = mean_absolute_error(results_df['Actual Score'], results_df['Predicted Score'])
 
        print(f"\nOverall Performance on Original Dataset:")
        print(f"RMSE: {rmse:.4f}")
        print(f"R2 Score: {r2:.4f}")
        print(f"MAE: {mae:.4f}")
    else:
        print("\nNo valid predictions were made.")
